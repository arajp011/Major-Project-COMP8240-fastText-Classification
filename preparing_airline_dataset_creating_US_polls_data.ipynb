{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Major Project: COMP8240**\n",
    "For this Project, we will use the ariline_sentiment Dataset to train the fastText classifier replacing the original data of stack exchange questions. The project follows the workflow of application of data science project and we will use a facebook's fastText classifier for the classification of collected data from twitter and reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the airline_sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('airline_sentiment.csv')\n",
    "df_check = pd.read_csv('airline_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the data of concern i.e. Tweets, sentiments and Twitter users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['airline_sentiment_confidence']\n",
    "del df['airline_sentiment_gold']\n",
    "del df['negativereason']\n",
    "del df['negativereason_confidence']\n",
    "del df['negativereason_gold']\n",
    "del df['retweet_count']\n",
    "del df['tweet_coord']\n",
    "del df['tweet_created']\n",
    "del df['tweet_id']\n",
    "del df['tweet_location']\n",
    "del df['user_timezone']\n",
    "del df['airline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the part of cleaning up the tweets before training the model on the airline_dataset. Because we want to replicate the original work of experiments, the data has to be of similar quality as of original data only the data annotations are changed to positive, negative an neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above funtion evaluates pyfunc over successive rows of the input arrays using the broadcasting rule of numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = np.vectorize(remove_pattern)(df['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = np.vectorize(remove_pattern)(df['text'], \"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets = 14639\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up the tweets we converted the tweet's text to lowercase, removed @ and Hashtags, removed website urls, removed special characters, removed white spaces, removed emojis/smileys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, max_tweets):\n",
    "    tweet = re.sub(r'(.*?)http.*?\\s?(.*?)', '',\n",
    "            df['text'][i], flags = re.MULTILINE)\n",
    "    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('rt', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ps.stem(word) for word in tweet\n",
    "        if not word in set(stopwords.words('english'))]\n",
    "    if tweet == []: continue\n",
    "    else:\n",
    "        tweet = ' '.join(tweet)\n",
    "        corpus.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda row:re.sub(r'http\\S+', '',row))\n",
    "df['text']=df['text'].apply(lambda row : row.lower())\n",
    "df['text']=df['text'].apply(lambda row : re.sub('\\d+','',row))\n",
    "df['text']=df['text'].apply(lambda row:re.sub(r\"[^A-Za-z0-9']+\", ' ',row))\n",
    "df['text']=df['text'].apply(lambda row:re.sub(r'\\s+', ' ',row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the comparison between a random original tweet and cleaned tweet see below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' your airline is awesome but your lax loft needs to step up its game for dirty tables and floors '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[73].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica your airline is awesome but your lax loft needs to step up its game. $40 for dirty tables and floors? http://t.co/hy0VrfhjHt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.iloc[73].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the data to be read by fastText classifier. Labels must start by the prefix _label_ which is how fasttext recognizes what a label or what a word is. Csv is then futher converted into to a txt file which is then fed into the fastText training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'] = '__label__' + df['airline_sentiment'].astype(str)\n",
    "df['lableled']=df['airline_sentiment'].astype(str) +' '+ df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['airline_sentiment']\n",
    "del df['text']\n",
    "del df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lableled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__neutral  what said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__positive  plus you've added commercia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__neutral  i didn't today must mean i n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__negative  it's really aggressive to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__negative  and it's a really big bad t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            lableled\n",
       "0                       __label__neutral  what said \n",
       "1  __label__positive  plus you've added commercia...\n",
       "2  __label__neutral  i didn't today must mean i n...\n",
       "3  __label__negative  it's really aggressive to b...\n",
       "4  __label__negative  and it's a really big bad t..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('airline_labelled.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input csv file name airline_labelled.csv\n",
      "Input text file nameairline_training.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_file = input('Input csv file name ')\n",
    "txt_file = input('Input text file name')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of new dataset using Tweepy a python library for accessing Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = '8nJfxRD8HMRNPAjy1PqWX89cI'\n",
    "consumer_key_secret = 'EQJEvLUisoH5DIAFBXZFqI4Du4QMMWIT7BGfsMx9D9s41fh2dI'\n",
    "access_token = '1251336102285242369-evdUh0gDQzpkw6yXJTSQBBEajfzlIS'\n",
    "access_token_secret = '8AJkJnuAMi97jNASBan7GsQi7flZM6c5HFvIcIZbx8fi2'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since my goal was to provide frame an opinion on 2020 US presidential elections. I have pulled tweets from the twitter handle ‘US Politics Polls’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets = 200\n",
    "screen_name = 'USPoliticsPoll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.user_timeline(screen_name = screen_name,\n",
    "                            tweet_mode = 'extended',\n",
    "                            count = max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below writes tweets to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = []\n",
    "for json_tweet in tweets:\n",
    "    list_of_dicts.append(json_tweet._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_json_data.txt', mode = 'w') as file:\n",
    "    file.write(json.dumps(list_of_dicts, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Tweets from the file generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "with open('tweet_json_data.txt', encoding='utf-8', mode = 'r') as file:\n",
    "    list_of_dicts = json.load(file)\n",
    "    for dict in list_of_dicts:\n",
    "    # Note we read selected information from the tweets.\n",
    "        list.append({'tweet_id': str(dict['id']),\n",
    "                'full_text': str(dict['full_text']),\n",
    "                'favorite_count': int(dict['favorite_count']),\n",
    "                'retweet_count': int(dict['retweet_count']),\n",
    "                'created_at': dict['created_at'],\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a dataframe using Tweets Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_new = pd.DataFrame(list, columns = ['tweet_id',\n",
    "                                    'full_text',\n",
    "                                    'favorite_count',\n",
    "                                    'retweet_count',\n",
    "                                    'created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the data of concern i.e. Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_new['created_at']\n",
    "del df_new['favorite_count']\n",
    "del df_new['tweet_id']\n",
    "del df_new['retweet_count']\n",
    "df_new.rename(columns={'full_text': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now cleaning up of the tweets and make them machine understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_patterns(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['text'] = np.vectorize(remove_patterns)(df_new['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['text'] = np.vectorize(remove_patterns)(df_new['text'], \"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of cleaning is replicated as is done with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets = 200\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, max_tweets):\n",
    "    tweet = re.sub(r'(.*?)http.*?\\s?(.*?)', '',\n",
    "            df_new['text'][i], flags = re.MULTILINE)\n",
    "    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('rt', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ps.stem(word) for word in tweet\n",
    "        if not word in set(stopwords.words('english'))]\n",
    "    if tweet == []: continue\n",
    "    else:\n",
    "        tweet = ' '.join(tweet)\n",
    "        corpus.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['text']=df_new['text'].apply(lambda row:re.sub(r'http\\S+', '',row))\n",
    "df_new['text']=df_new['text'].apply(lambda row : row.lower())\n",
    "df_new['text']=df_new['text'].apply(lambda row : re.sub('\\d+','',row))\n",
    "df_new['text']=df_new['text'].apply(lambda row:re.sub(r\"[^A-Za-z0-9']+\", ' ',row))\n",
    "df_new['text']=df_new['text'].apply(lambda row:re.sub(r'\\s+', ' ',row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Csv to get human annotations using Qualtrics survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('Polls.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the annotated new data and convert them into fastText readable form so that we can test our model over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Polls1.csv')\n",
    "df2 = pd.read_csv('Polls2.csv')\n",
    "df3 = pd.read_csv('Polls3.csv')\n",
    "df4 = pd.read_csv('Polls4.csv')\n",
    "df5 = pd.read_csv('Polls5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['label'] = '__label__' + df1['sentiment'].astype(str)\n",
    "df1['lableled']=df1['label'].astype(str) +' '+ df1['text']\n",
    "df2['label'] = '__label__' + df2['sentiment'].astype(str)\n",
    "df2['lableled']=df2['label'].astype(str) +' '+ df2['text']\n",
    "df3['label'] = '__label__' + df3['sentiment'].astype(str)\n",
    "df3['lableled']=df3['label'].astype(str) +' '+ df3['text']\n",
    "df4['label'] = '__label__' + df4['sentiment'].astype(str)\n",
    "df4['lableled']=df4['label'].astype(str) +' '+ df4['text']\n",
    "df5['label'] = '__label__' + df5['sentiment'].astype(str)\n",
    "df5['lableled']=df5['label'].astype(str) +' '+ df5['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1['sentiment']\n",
    "del df1['text']\n",
    "del df1['label']\n",
    "del df2['sentiment']\n",
    "del df2['text']\n",
    "del df2['label']\n",
    "del df3['sentiment']\n",
    "del df3['text']\n",
    "del df3['label']\n",
    "del df4['sentiment']\n",
    "del df4['text']\n",
    "del df4['label']\n",
    "del df5['sentiment']\n",
    "del df5['text']\n",
    "del df5['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('Poll1.csv', index = False, header=False)\n",
    "df2.to_csv('Poll2.csv', index = False, header=False)\n",
    "df3.to_csv('Poll3.csv', index = False, header=False)\n",
    "df4.to_csv('Poll4.csv', index = False, header=False)\n",
    "df5.to_csv('Poll5.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input csv file name Poll1.csv\n",
      "Input text file namePoll1.txt\n"
     ]
    }
   ],
   "source": [
    "csv_file = input('Input csv file Poll1.csv ')\n",
    "txt_file = input('Input text file Poll1.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input csv file Poll2.csv Poll2.csv\n",
      "Input text file Poll2.txtPoll2.txt\n"
     ]
    }
   ],
   "source": [
    "csv_file = input('Input csv file Poll2.csv ')\n",
    "txt_file = input('Input text file Poll2.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input csv file Poll3.csv Poll3.csv\n",
      "Input text file Poll3.txtPoll3.txt\n"
     ]
    }
   ],
   "source": [
    "csv_file = input('Input csv file Poll3.csv ')\n",
    "txt_file = input('Input text file Poll3.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input csv file Poll4.csv Poll4.csv\n",
      "Input text file Poll4.txtPoll4.txt\n"
     ]
    }
   ],
   "source": [
    "csv_file = input('Input csv file Poll4.csv ')\n",
    "txt_file = input('Input text file Poll4.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input csv file Poll5.csv Poll5.csv\n",
      "Input text file Poll5.txtPoll5.txt\n"
     ]
    }
   ],
   "source": [
    "csv_file = input('Input csv file Poll5.csv ')\n",
    "txt_file = input('Input text file Poll5.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
